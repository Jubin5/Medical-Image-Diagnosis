{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import  ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D,Concatenate,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9KVJ25C3zFpD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KSyipUcYy2t6"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/chest_xray.zip\"\n",
        "extract_path = \"/content/chest_xray_data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for folder in os.listdir(extract_path):\n",
        "    print(\"📂\", folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoYmLQAozhln",
        "outputId": "778ff7b8-d925-4488-d952-97f2fb506701"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 test\n",
            "📂 train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/chest_xray_data\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# Image Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # we'll split here\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper to wrap generator output with proper structure\n",
        "def duplicate_input(generator):\n",
        "    for x, y in generator:\n",
        "        yield (x, x), y  # Create 2 inputs for ensemble\n",
        "\n",
        "\n",
        "# Define output_signature here\n",
        "output_signature = (\n",
        "    (\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
        "    ),\n",
        "    tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create dual input test set\n",
        "test_dual_input = tf.data.Dataset.from_generator(\n",
        "    lambda: duplicate_input(test_gen),\n",
        "    output_signature=(\n",
        "        (\n",
        "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
        "        ),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "# Wrap the generator with tf.data.Dataset\n",
        "train_dual_input = tf.data.Dataset.from_generator(\n",
        "    lambda: duplicate_input(train_gen),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "val_dual_input = tf.data.Dataset.from_generator(\n",
        "    lambda: duplicate_input(val_gen),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Data Augmentation only on training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.15  # you can adjust this if needed\n",
        ")\n",
        "\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    '/content/chest_xray_data/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Base models\n",
        "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom heads\n",
        "vgg_out = GlobalAveragePooling2D()(vgg.output)\n",
        "resnet_out = GlobalAveragePooling2D()(resnet.output)\n",
        "\n",
        "\n",
        "combined = Concatenate()([vgg_out, resnet_out])\n",
        "combined = Dropout(0.5)(combined)  # Dropout added here after concatenation\n",
        "combined = Dense(256, activation='relu')(combined)\n",
        "combined = Dropout(0.5)(combined)  # Dropout after Dense layer\n",
        "output = Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "\n",
        "model = Model(inputs=[vgg.input, resnet.input], outputs=output)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "# Note: We'll use two separate generators for VGG and ResNet if needed, but this model assumes same input\n",
        "model.fit(\n",
        "    train_dual_input,\n",
        "    validation_data=val_dual_input,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "model.evaluate(test_dual_input, steps=len(test_gen))\n",
        "\n",
        "# Save model\n",
        "model.save('ensemble_cxr_model.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmjxW9dM3GN7",
        "outputId": "bb4a9da5-9efa-4871-f896-05a0ad95aa3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4187 images belonging to 2 classes.\n",
            "Found 1045 images belonging to 2 classes.\n",
            "Found 4448 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Epoch 1/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 907ms/step - accuracy: 0.6442 - loss: 0.7252 - val_accuracy: 0.7426 - val_loss: 0.4957\n",
            "Epoch 2/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 919ms/step - accuracy: 0.7188 - loss: 0.5854 - val_accuracy: 0.7445 - val_loss: 0.4476\n",
            "Epoch 3/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.7480 - loss: 0.5140 - val_accuracy: 0.7646 - val_loss: 0.4048\n",
            "Epoch 4/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 779ms/step - accuracy: 0.7680 - loss: 0.4840 - val_accuracy: 0.8593 - val_loss: 0.3798\n",
            "Epoch 5/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.7829 - loss: 0.4532 - val_accuracy: 0.8852 - val_loss: 0.3425\n",
            "Epoch 6/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 768ms/step - accuracy: 0.7841 - loss: 0.4420 - val_accuracy: 0.8938 - val_loss: 0.3209\n",
            "Epoch 7/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8108 - loss: 0.4102 - val_accuracy: 0.9053 - val_loss: 0.3122\n",
            "Epoch 8/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 778ms/step - accuracy: 0.8232 - loss: 0.3845 - val_accuracy: 0.8947 - val_loss: 0.2824\n",
            "Epoch 9/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8260 - loss: 0.3779 - val_accuracy: 0.8976 - val_loss: 0.3080\n",
            "Epoch 10/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 776ms/step - accuracy: 0.8371 - loss: 0.3670 - val_accuracy: 0.9053 - val_loss: 0.2836\n",
            "Epoch 11/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8422 - loss: 0.3473 - val_accuracy: 0.9110 - val_loss: 0.2670\n",
            "Epoch 12/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 792ms/step - accuracy: 0.8518 - loss: 0.3274 - val_accuracy: 0.8909 - val_loss: 0.2989\n",
            "Epoch 13/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 782ms/step - accuracy: 0.8404 - loss: 0.3515 - val_accuracy: 0.9081 - val_loss: 0.2688\n",
            "Epoch 14/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 785ms/step - accuracy: 0.8369 - loss: 0.3547 - val_accuracy: 0.9024 - val_loss: 0.2842\n",
            "Epoch 15/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8495 - loss: 0.3407 - val_accuracy: 0.9129 - val_loss: 0.2545\n",
            "Epoch 16/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 808ms/step - accuracy: 0.8600 - loss: 0.3242 - val_accuracy: 0.9072 - val_loss: 0.2604\n",
            "Epoch 17/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 798ms/step - accuracy: 0.8502 - loss: 0.3207 - val_accuracy: 0.9091 - val_loss: 0.2446\n",
            "Epoch 18/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 802ms/step - accuracy: 0.8609 - loss: 0.3194 - val_accuracy: 0.9139 - val_loss: 0.2423\n",
            "Epoch 19/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 791ms/step - accuracy: 0.8620 - loss: 0.3052 - val_accuracy: 0.9148 - val_loss: 0.2442\n",
            "Epoch 20/20\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 783ms/step - accuracy: 0.8671 - loss: 0.3052 - val_accuracy: 0.9120 - val_loss: 0.2533\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 946ms/step - accuracy: 0.7932 - loss: 0.4458\n"
          ]
        }
      ]
    }
  ]
}